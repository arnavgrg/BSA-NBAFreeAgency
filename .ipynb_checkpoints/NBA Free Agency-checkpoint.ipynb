{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Free Agency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to determine whether the top 10 NBA free agents (or players who opt for player option in their contracts) are likely stay or leave a team during free agency or the mid-season trade deadline. This will be determined using NLP techniques on phrases extracted from tweets, news, interviews, polls, basketball stats, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 NBA Free Agents 2019\n",
    "According to SBNation and ESPN these players are: <br>\n",
    "Link: https://www.sbnation.com/nba/2018/7/30/17616436/nba-free-agency-2019-list-kevin-durant-kyrie-irving\n",
    "\n",
    "1. Kevin Durant\n",
    "2. Kawhi Leonard\n",
    "3. Kyrie Irving\n",
    "4. Jimmy Butler \n",
    "5. Klay Thompson\n",
    "6. DeMarcus Cousins\n",
    "7. Al Horford\n",
    "8. Kemba Walker\n",
    "9. Khris Middleton\n",
    "10. Eric Bledsoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/raghavagovil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/raghavagovil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/raghavagovil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import string\n",
    "\n",
    "#Text Processing\n",
    "#Download package for word_tokenize and lemmatize NLTK functions \n",
    "#  1. punkt\n",
    "#  2. stopwords\n",
    "#  3. wordnet\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the links of Google\n",
    "def return_links(user_query):\n",
    "    #Initialize empty array to store all links scraped from google query\n",
    "    links = []\n",
    "    google_search = \"https://www.google.com/search?sclient=psy-ab&client=ubuntu&hs=k5b&channel=fs&biw=1366&bih=648&noj=1&q=\" + user_query\n",
    "    r = requests.get(google_search)\n",
    "    #If google query is valid and returns 200 status (True)\n",
    "    if r.ok:\n",
    "        #Parse the returned object into an HTML format that bs4 understands\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        #Go through each item in the google page, and save the green link beneath the\n",
    "        #page header and append it to the empty list\n",
    "        for item in soup.find_all('h3', attrs={'class' : 'r'}):\n",
    "            links.append(item.a['href'][7:])\n",
    "    #If status fails/404 error/page does not load correctly/invalid URL retrieved\n",
    "    else:\n",
    "        #Find query_errors and save the URL in an error log\n",
    "        f = open(\"Error-Logs/query_errors.txt\",\"a+\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(user_query)\n",
    "        f.close()\n",
    "    return links\n",
    "\n",
    "#Cleaning the extracted links they become valid \n",
    "def clean_links(links):\n",
    "    '''\n",
    "    This function cleans a link or a list of extracted links\n",
    "    '''\n",
    "    #Go through all the links and remove the extensions of the link\n",
    "    #after the '&' and '%' symbols within each URL. \n",
    "    # -> Needed to create valid URLs from scraped URLs\n",
    "    for i in range(0, len(links)):\n",
    "        x = links[i].find('&')\n",
    "        if x != -1:\n",
    "            links[i] = links[i][:x]\n",
    "        for i in range(0, len(links)):\n",
    "            x = links[i].find('%')\n",
    "            if x != -1:\n",
    "                links[i] = links[i][:x]\n",
    "    #Remove the initial invalid google search query link\n",
    "    for i in range(0, len(links)):\n",
    "        #Finding link that starts with '?' after cleaning\n",
    "        test = re.findall(r\"\\?\", links[i])\n",
    "        #Converting list to string\n",
    "        str1 = ''.join(test)\n",
    "        if(str1 == '?'):\n",
    "            links.remove(links[i])\n",
    "            break\n",
    "    for i in range(len(links)):\n",
    "        print(i+1,links[i])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Processing:\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Data Processing consists of: \n",
    "    1. Converting text to lowercase\n",
    "    2. Extracting alpha-numeric\n",
    "    3. Removing stop words\n",
    "    4. Removes punctuations\n",
    "    5. Lemmatizes the given text\n",
    "    6. Stems the given text\n",
    "    7. Removes potential HTML markup tags\n",
    "    8. Remove white spaces\n",
    "    '''\n",
    "    #1. Convert to lower case\n",
    "    text = text.lower()\n",
    "    #5. Lemmatizing the text\n",
    "    lemma = WordNetLemmatizer()\n",
    "    normalized = \" \".join(lemma.lemmatize(word, pos = \"v\") for word in text.split())\n",
    "    #Removing White spaces\n",
    "    normalized = normalized.replace('\\d+', '')\n",
    "    normalized = normalized.strip()\n",
    "    #Tokenize and extract words that are alpha-numeric\n",
    "    tokens = word_tokenize(normalized)\n",
    "    cleaned = [word for word in tokens if word.isalpha()]\n",
    "    #Create a dictionary of stem-words such as \"at\" and \"the\"\n",
    "    #that don't contribute to meaning and remove them from the list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in cleaned if not w in stop_words]\n",
    "    #Remove punctuations\n",
    "    exclude = set(string.punctuation)\n",
    "    punc_free = [ch for ch in stop_words if ch not in exclude]\n",
    "    #Stem words to root words if/where possible\n",
    "    #porter = PorterStemmer()\n",
    "    #stemmed = [porter.stem(word) for word in punc_free]\n",
    "    #Remove common html markup words\n",
    "    html_words = ['html','http','https','.com','.org','.edu', \n",
    "                  'img', 'href', 'span', 'b', 'u']\n",
    "    words = [w for w in punc_free if not w in html_words]\n",
    "    return words\n",
    "\n",
    "#Writing the processed data to csv files\n",
    "count = 0\n",
    "def save_text(links):\n",
    "    global count\n",
    "    for i in range(0, len(links)):\n",
    "        r = requests.get(links[i])\n",
    "        if r.ok:\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            text = soup.find_all('p')\n",
    "            page_text = \"\"\n",
    "            for item in text:\n",
    "                str_contents = str(item.contents)\n",
    "                len_contents = len(str_contents)\n",
    "                page_text += str_contents[1:len_contents-1]\n",
    "            text = clean_text(page_text)\n",
    "            f = open(\"Excerpts/excerpt{}.csv\".format(count),\"w+\")\n",
    "            f.write(str(links[i])+\"\\n\\n\")\n",
    "            f.write(str(text))\n",
    "            f.close()\n",
    "            page_text = \"\"\n",
    "            count += 1\n",
    "        else:\n",
    "            f = open(\"Error-Logs/text_saving_errors.txt\",\"a+\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(links[i])\n",
    "            f.close()\n",
    "    print(\"\\n{} files saved.\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://twitter.com/kingjames\n",
      "2 https://twitter.com/kingjames/status/1052698329795026944\n",
      "3 https://twitter.com/hashtag/lebronjames\n",
      "4 https://twitter.com/FOXSports/status/1060580256933793792\n",
      "5 https://twitter.com/hashtag/lebron\n",
      "6 https://www.usatoday.com/story/sports/nba/lakers/2018/10/29/lebron-james-michael-jordan-matchup/1804629002/\n",
      "7 https://www.nbcsports.com/philadelphia/the700level/ben-simmons-sets-twitter-ablaze-tweet-lebron-james\n",
      "8 https://www.cbsnews.com/pictures/the-biggest-twitter-accounts-on-earth/4/\n",
      "9 https://www.instagram.com/kingjames/\n",
      "\n",
      "9 files saved.\n"
     ]
    }
   ],
   "source": [
    "#Extracting links\n",
    "a = return_links(\"Lebron James Twitter\")\n",
    "#Cleaning Data\n",
    "a = clean_links(a)\n",
    "#Writing to csv files\n",
    "save_text(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
