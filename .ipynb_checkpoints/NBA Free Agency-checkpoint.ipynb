{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Free Agency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to determine whether the top 10 NBA free agents (or players who opt for player option in their contracts) are likely stay or leave a team during free agency or the mid-season trade deadline. This will be determined using NLP techniques on phrases extracted from tweets, news, interviews, polls, basketball stats, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 NBA Free Agents 2019\n",
    "According to SBNation and ESPN these players are: <br>\n",
    "Link: https://www.sbnation.com/nba/2018/7/30/17616436/nba-free-agency-2019-list-kevin-durant-kyrie-irving\n",
    "\n",
    "1. Kevin Durant\n",
    "2. Kawhi Leonard\n",
    "3. Kyrie Irving\n",
    "4. Jimmy Butler \n",
    "5. Klay Thompson\n",
    "6. DeMarcus Cousins\n",
    "7. Al Horford\n",
    "8. Kemba Walker\n",
    "9. Khris Middleton\n",
    "10. Eric Bledsoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/arnavgarg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arnavgarg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/arnavgarg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import urllib3\n",
    "import string\n",
    "import time \n",
    "\n",
    "#Text Processing\n",
    "#Download package for word_tokenize and lemmatize NLTK functions \n",
    "#  1. punkt\n",
    "#  2. stopwords\n",
    "#  3. wordnet\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "def return_links(user_query):\n",
    "    '''\n",
    "    Function takes in a string as a query, \n",
    "    and scrapes the links of the top 10 websites \n",
    "    for this query on Google.\n",
    "    \n",
    "    Returns: A list of URLs\n",
    "    '''\n",
    "    #Initialize empty array to store all links scraped from google query\n",
    "    links = []\n",
    "    google_search = \"https://www.google.com/search?sclient=psy-ab&client=ubuntu&hs=k5b&channel=fs&biw=1366&bih=648&noj=1&q=\" + user_query\n",
    "    r = requests.get(google_search)\n",
    "    #If google query is valid and returns 200 status (True)\n",
    "    if r.ok:\n",
    "        #Parse the returned object into an HTML format that bs4 understands\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        #Go through each item in the google page, and save the green link beneath the\n",
    "        #page header and append it to the empty list\n",
    "        for item in soup.find_all('h3', attrs={'class' : 'r'}):\n",
    "            links.append(item.a['href'][7:])\n",
    "    #If status fails/404 error/page does not load correctly/invalid URL retrieved\n",
    "    else:\n",
    "        #Find query_errors and save the URL in an error log\n",
    "        f = open(\"Error-Logs/query_errors.txt\",\"a+\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(user_query)\n",
    "        f.close()\n",
    "    return links\n",
    "\n",
    "def clean_links(links):\n",
    "    '''\n",
    "    This function takes a list of links, cleans each link \n",
    "    so that they become valid URLs. It also eliminates \n",
    "    most invalid links/URLs.\n",
    "    \n",
    "    Returns: A list of valid URLs.\n",
    "    '''\n",
    "    #Go through all the links and remove the extensions of the link\n",
    "        #after the '&' and '%' symbols within each URL. \n",
    "    #Remove the initial invalid google search query link\n",
    "    for i in range(0, len(links)-1):\n",
    "        #Returns index where character is found\n",
    "        x = links[i].find('&')\n",
    "        y = links[i].find('%')\n",
    "        flag = False\n",
    "        if x != -1:\n",
    "            print(\"Found x:\",x)\n",
    "            links[i] = links[i][:x]\n",
    "            flag = True\n",
    "        if y != -1 and not flag:\n",
    "            print(\"Found y:\",y)\n",
    "            links[i] = links[i][:y]\n",
    "        #Finding link that starts with '?' after cleaning\n",
    "        test = re.findall(r\"\\?\", links[i])\n",
    "        #Converting list to string\n",
    "        str1 = ''.join(test)\n",
    "        if(str1 == '?'):\n",
    "            links.remove(links[i])\n",
    "        #Print link to output\n",
    "        print(i+1,links[i])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    '''\n",
    "    Helper function to clean html tags.\n",
    "    \n",
    "    Input: Block of raw HTML text in lower case.\n",
    "    Returns: Text almost entirely free of HTML tags.\n",
    "    '''\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    cleantext = re.sub('https', '', cleantext)\n",
    "    return cleantext\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Function takes a text extract, and processes it.\n",
    "    \n",
    "    Data Processing consists of: \n",
    "    1. Converting text to lowercase\n",
    "    2. Extracting alpha-numeric\n",
    "    3. Removing stop words\n",
    "    4. Removes punctuations\n",
    "    5. Lemmatizes the given text\n",
    "    6. Stems the given text\n",
    "    7. Removes potential HTML markup tags\n",
    "    8. Remove white spaces\n",
    "    \n",
    "    Returns: A list of tokenized, processed words.\n",
    "    \n",
    "    Optionally add stemming:\n",
    "    #Stem words to root words if/where possible\n",
    "    #porter = PorterStemmer()\n",
    "    #stemmed = [porter.stem(word) for word in punc_free]\n",
    "    #Remove common html markup words\n",
    "    '''\n",
    "    #Convert to lower case\n",
    "    text = text.lower()\n",
    "    #Remove HTML tags\n",
    "    text = cleanhtml(text)\n",
    "    #Split text, and lemmatize each word\n",
    "    lemma = WordNetLemmatizer()\n",
    "    normalized = \" \".join(lemma.lemmatize(word, pos = \"v\") for word in text.split())\n",
    "    #Replace all digits with blank spaces\n",
    "    normalized = normalized.replace('\\d+', '')\n",
    "    #Remove all white spaces (strip removes white spaces by default)\n",
    "    normalized = normalized.strip()\n",
    "    #Tokenize the text \n",
    "    tokenized = word_tokenize(normalized)\n",
    "    #Extract words that are alpha and removes punctuations\n",
    "    cleaned = [word for word in tokenized if word.isalpha()]\n",
    "    #Create a dictionary of stem-words such as \"at\" and \"the\"\n",
    "    #that don't contribute to meaning and remove them from the list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in cleaned if not w in stop_words]\n",
    "    return words\n",
    "\n",
    "count = 0\n",
    "def save_text(links):\n",
    "    '''\n",
    "    Function takes in a set of links and extracts the text\n",
    "    content from each website. It then processes this text\n",
    "    content and stores into a csv file titled \"excerpt\".\n",
    "    \n",
    "    Returns: N/A\n",
    "    '''\n",
    "    global count\n",
    "    for i in range(0, len(links)):\n",
    "        r = requests.get(links[i])\n",
    "        if r.ok:\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            text = soup.find_all('p')\n",
    "            page_text = \"\"\n",
    "            for item in text:\n",
    "                str_contents = str(item.contents)\n",
    "                len_contents = len(str_contents)\n",
    "                page_text += str_contents[1:len_contents-1]\n",
    "            text = str(clean_text(page_text))\n",
    "            f = open(\"Excerpts/excerpt{}.csv\".format(count),\"w+\")\n",
    "            f.write(str(links[i])+\"\\n\\n\"+text)\n",
    "            f.close()\n",
    "            page_text = \"\"\n",
    "            count += 1\n",
    "        else:\n",
    "            f = open(\"Error-Logs/text_saving_errors.txt\",\"a+\")\n",
    "            f.write(str(links[i])+\"\\n\")\n",
    "            f.close()\n",
    "    print(\"\\n{} files saved.\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found x: 15\n",
      "1 https://en.wikipedia.org/wiki/LeBron_James&sa=U&ved=0ahUKEwjGnpne-creAhXiFzQIHUNvCw8QFggdMAM&usg=AOvVaw2ytH8Siwq29xGmfpEwMOZt\n",
      "Found x: 53\n",
      "2 http://www.espn.com/nba/player/_/id/1966/lebron-james\n",
      "Found x: 61\n",
      "3 https://www.basketball-reference.com/players/j/jamesle01.html\n",
      "Found x: 27\n",
      "4 http://www.lebronjames.com/\n",
      "Found x: 29\n",
      "5 https://twitter.com/kingjames\n",
      "Found x: 71\n",
      "6 https://sports.yahoo.com/rumor-lebron-james-called-favor-150028976.html\n",
      "Found x: 42\n",
      "7 https://sports.yahoo.com/nba/players/3704/\n",
      "Found x: 68\n",
      "8 https://www.cbssports.com/nba/players/playerpage/400553/lebron-james\n",
      "Found x: 130\n",
      "9 https://www.express.co.uk/sport/othersport/1043601/NBA-news-Golden-State-Warriors-DeMarcus-Cousins-LeBron-James-Los-Angeles-Lakers\n",
      "\n",
      "8 files saved.\n",
      "Runtime: 8.629467964172363\n"
     ]
    }
   ],
   "source": [
    "#Extracting links\n",
    "a = return_links(\"Lebron James\")\n",
    "#Cleaning Data\n",
    "a = clean_links(a)\n",
    "#Writing to csv files\n",
    "save_text(a)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Runtime:\",end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
