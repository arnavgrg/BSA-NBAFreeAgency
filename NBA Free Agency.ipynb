{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Free Agency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to determine whether the top 10 NBA free agents (or players who opt for player option in their contracts) are likely stay or leave a team during free agency or the mid-season trade deadline. This will be determined using NLP techniques on phrases extracted from tweets, news, interviews, polls, basketball stats, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 NBA Free Agents 2019\n",
    "According to SBNation and ESPN these players are: <br>\n",
    "Link: https://www.sbnation.com/nba/2018/7/30/17616436/nba-free-agency-2019-list-kevin-durant-kyrie-irving\n",
    "\n",
    "1. Kevin Durant\n",
    "2. Kawhi Leonard\n",
    "3. Kyrie Irving\n",
    "4. Jimmy Butler \n",
    "5. Klay Thompson\n",
    "6. DeMarcus Cousins\n",
    "7. Al Horford\n",
    "8. Kemba Walker\n",
    "9. Khris Middleton\n",
    "10. Eric Bledsoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/arnavgarg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arnavgarg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import string\n",
    "\n",
    "#Text Processing\n",
    "#Download package for word_tokenize NLTK functions \n",
    "#  1. punkt\n",
    "#  2. stopwords\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_links(user_query):\n",
    "    links = []\n",
    "    google_search = \"https://www.google.com/search?sclient=psy-ab&client=ubuntu&hs=k5b&channel=fs&biw=1366&bih=648&noj=1&q=\" + user_query\n",
    "    r = requests.get(google_search)\n",
    "    if r.ok:\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for item in soup.find_all('h3', attrs={'class' : 'r'}):\n",
    "            links.append(item.a['href'][7:])\n",
    "    else:\n",
    "        f = open(\"Error-Logs/query_errors.txt\",\"a+\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(user_query)\n",
    "        f.close()\n",
    "    return links\n",
    "\n",
    "def clean_links(links):\n",
    "    for i in range(0, len(links)):\n",
    "        x = links[i].find('&')\n",
    "        if x != -1:\n",
    "            links[i] = links[i][:x]\n",
    "        for i in range(0, len(links)):\n",
    "            x = links[i].find('%')\n",
    "            if x != -1:\n",
    "                links[i] = links[i][:x]\n",
    "    #Remove invalid google search query link\n",
    "    links.remove(links[0])\n",
    "    for i in range(len(links)):\n",
    "        print(i+1,links[i])\n",
    "    return links    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Convert to lower case and tokenize\n",
    "    #Only extract words that are alpha-numeric\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    cleaned = [word for word in tokens if word.isalpha()]\n",
    "    #Create a dictionary of stem-words such as \"at\" and \"\n",
    "    #the\" that don't contribute to meaning and remove them from the list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in cleaned if not w in stop_words]\n",
    "    #Stem words to root words if/where possible\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    #Remove common html markup words\n",
    "    html_words = ['html','http','https','.com','.org','.edu', \n",
    "                  'img', 'href', 'span', 'b', 'u']\n",
    "    words = [w for w in stemmed if not w in html_words]\n",
    "    return words\n",
    "    \n",
    "count = 0\n",
    "def save_text(links):\n",
    "    global count\n",
    "    for i in links:\n",
    "        r = requests.get(i)\n",
    "        if r.ok:\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            text = soup.find_all('p')\n",
    "            page_text = \"\"\n",
    "            for item in text:\n",
    "                str_contents = str(item.contents)\n",
    "                len_contents = len(str_contents)\n",
    "                page_text += str_contents[1:len_contents-1]\n",
    "            text = clean_text(page_text)\n",
    "            f = open(\"Excerpts/excerpt{}.csv\".format(count),\"w+\")\n",
    "            f.write(str(i)+\"\\n\\n\")\n",
    "            f.write(str(text))\n",
    "            f.close()\n",
    "            page_text = \"\"\n",
    "            count += 1\n",
    "        else:\n",
    "            f = open(\"Error-Logs/text_saving_errors.txt\",\"a+\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(i)\n",
    "            f.close()\n",
    "    print(\"\\n{} files saved.\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.wsj.com/articles/SB10001424052748704111704575355162960155010\n",
      "2 http://www.espn.com/video/clip\n",
      "3 https://www.cleveland.com/cavs/index.ssf/2014/07/lebron_james_cavaliers_heat_decision_2.html\n",
      "4 https://www.sun-sentinel.com/sports/miami-heat/fl-sp-miami-heat-lebron-james-s20180609-story.html\n",
      "5 https://www.nbcnews.com/think/opinion/lebron-james-decision-leave-cavaliers-lakers-isn-t-surprising-his-ncna888331\n",
      "6 https://bleacherreport.com/articles/2785744-lebron-james-decision-and-the-most-shocking-free-agent-signings-in-nba-history\n",
      "7 https://www.theguardian.com/sport/2010/jul/09/lebron-james-joins-miami-heat\n",
      "8 https://www.telegraph.co.uk/sport/othersports/basketball/7880779/LeBron-James-announces-move-to-Miami-Heat-as-Cavaliers-fans-burn-jerseys-in-anger.html\n",
      "9 https://abcnews.go.com/GMA/lebron-james-cavs-owner-dan-gilbert-writes-letter/story\n",
      "\n",
      "7 files saved.\n"
     ]
    }
   ],
   "source": [
    "a = return_links(\"Lebron James' shocking decision to join Miami Heat\")\n",
    "a = clean_links(a)\n",
    "save_text(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
